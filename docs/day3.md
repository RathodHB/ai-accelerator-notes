# Day 3 - **Image & Video Manipulation & clone Generation using AI**

**Theme**: *Creative Storytelling & Character Cloning Using Diffusion Models and Generative AI*

---

## **1. Introduction to the Day’s Focus**

Day 3 focused on the **creative application of diffusion models** for visual storytelling. This built upon the learnings from previous days, which covered text and voice models. The session transitioned into enabling participants to create not only AI-generated images and videos but to **embed themselves as consistent characters (clones)** within those narratives using advanced AI tools.

---

## **2. Conceptual Framework: AI Generalist in a Creative Workflow**

Participants were encouraged to think like “**AI generalists**,” capable of:

- Communicating effectively across creative, sales, and marketing teams.
- Using AI tools to solve content and communication problems.
- Building **end-to-end workflows**—from ideation to production—leveraging generative AI.

---

## **3. Tools Introduced and Compared**

### **3.1. Replicate**

A pay-per-use AI platform hosting multiple AI models (text, image, video).

- No monthly fee: users pay only for what they generate.
- Models explored:
    - **Google Imagen v3 & v4**: High fidelity and strong prompt adherence.
    - **Flux Pro Ultra**: Open-source, excellent for realistic visuals.
    - **Recraft v3**: Good styling but weaker prompt alignment.

### **3.2. RunwayML**

Primary platform for:

- Generating **consistent characters** using “**reference images**.”
- Creating **cinematic visuals** and later exporting to video.
- Supports up to **three reference images**, e.g.:
    - Scene background (asset)
    - User photo (face)
    - Costume or visual element (outfit)

### **3.3. Google Imagen (via Replicate)**

- Found to be particularly strong in **prompt adherence** and realism.
- **Imagen v3** often outperformed v4 in practical examples.

### **3.4. Prompting Tools**

- Used ChatGPT (and prompt builder GPTs) to generate precise image prompts.
- Reinforced the importance of **short, directive prompts** (especially in Runway).

---

## **4. Key Concepts and Demonstrations**

### **4.1. Storyboarding**

- Before generating visuals, always build a **scene-by-scene storyboard**.
- Outline includes:
    - Scene setting (e.g., Neolithic age, Gladiator arena)
    - Required assets (backgrounds, props)
    - Character placement and roles

### **4.2. Asset Creation**

- Assets refer to **backgrounds or scenes** that don't include the user yet.
- Built using Replicate by generating environmental shots (e.g., berry foraging scene, ancient Rome arena).
- Compared outputs across multiple models to choose the most cinematic.

### **4.3. Cloning via References in Runway**

- Inserted the **user’s face** into the scene using references.
- Used a separate image of the user, even if dressed casually, to preserve facial identity.
- Demonstrated how Runway adjusts **hair, beard, costume, and body posture** automatically.

### **4.4. Costumes and Props**

- Created custom costumes (e.g., gladiator armor) as white-background images.
- Used the third reference slot in Runway to ensure **costume consistency**.
- Blocked elements (like helmets) using Runway’s **image editor** to refine visuals.

### **4.5. Prompt Engineering for Realism**

- Incorporated height, age, facial hair, lighting, and clothing cues into prompts.
- Added environmental enhancements (e.g., **volcano eruptions, mammoths** in the background) to build immersion.

---

## **5. Iterative Workflow and Prompt Evolution**

Each generation was fine-tuned through:

- Prompt revision (e.g., adding descriptors like “cinematic,” “great detail”).
- Visual editing (blocking unwanted image areas).
- Controlled reinforcement (e.g., repeating key constraints like “face must match reference 2”).

This approach allowed the team to:

- Move from wide shots to mid-shots and close-ups.
- Maintain **facial, costume, and environmental continuity** across all shots.
- Easily generate multiple consistent scenes for a video.

---

## **6. Expression Control**

- Demonstrated how to control expressions (e.g., sad, angry, delighted) using prompts.
- Emphasized that **Runway’s image generation** supports expression control, but it’s more effective in **video generation**.

---

## **7. Video Generation Preparation**

- Once images were finalized, participants were shown how to:
    - **Export and reuse them as references** in tools like **Kling** or **Runway Video**.
    - Ensure consistency in visuals when generating scenes in sequence.

---

## **8. Case Study: Outscale Ad**

- Shared a case study showing how a full ad was created using this pipeline:
    - Concept to delivery in **48 hours** (compared to traditional 4–6 weeks).
    - Used generative assets, cloned actor, and consistent narrative.
    - Demonstrated real-world application for branded content.

---

## **9. Commercial and Strategic Implications**

- Highlighted how AI drastically reduces:
    - **Production time**
    - **Team dependency** (fewer editors, designers needed)
    - **Cost overheads**
- Encouraged participants to:
    - **Monetize** their workflows through social media and client work.
    - Establish content studios or personal brand engines using AI.

---

## **10. Key Takeaways**

- **Reference-based workflows** are replacing LoRA training for image consistency.
- **Prompt quality and asset planning** are critical to professional output.
- **Runway and Replicate** provide powerful tools for prototyping and storytelling.
- This kind of cloning and storytelling has strong potential for **viral content** and **AI-driven businesses**.
